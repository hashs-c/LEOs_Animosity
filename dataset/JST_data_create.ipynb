{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sdey/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sdey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class leos_member_basic(object):\n",
    "    def __init__(self, region, ceo, state, name, title, office, got, party, twitter_handle, personalhandle, leo):\n",
    "        self.region = region\n",
    "        self.ceo = ceo\n",
    "        self.name = name\n",
    "        self.title = title\n",
    "        self.office = office\n",
    "        self.got = got\n",
    "        self.party = party\n",
    "        self.twitter_handle = twitter_handle\n",
    "        self.personalhandle = personalhandle\n",
    "        self.author_id = None\n",
    "        self.leo = leo\n",
    "        self.twitter_id_set = set()\n",
    "        \n",
    "class leos_member_twitter(leos_member_basic):\n",
    "    def __init__(self, region, ceo, state, name, title, office, party, got, twitter_handle, personalhandle, leo):\n",
    "        super(leos_member_twitter, self).__init__(region, ceo, state, name, title, office, got, party, twitter_handle, personalhandle, leo)\n",
    "        self.raw_twitter_record = []\n",
    "    \n",
    "    def get_tweets(self,handle):\n",
    "        # print('start collecting tweets for handle: ' + handle)\n",
    "        max_id = None\n",
    "        for _ in range(16):\n",
    "            try:\n",
    "                t = api.GetUserTimeline(screen_name=handle, count=200, max_id=max_id, include_rts=True)\n",
    "                tweets = [i.AsDict() for i in t]\n",
    "                for t in tweets:\n",
    "                    try:\n",
    "                        if t[\"id\"] not in self.twitter_id_set:\n",
    "                            max_id = t[\"id\"]\n",
    "                            self.raw_twitter_record.append(t)\n",
    "                            self.twitter_id_set.add(t[\"id\"])\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'youd', 'doesnt', 'youll', 'will', 'wont', 'aint', 'ith', 'their', \"they're\", 'is', 'was', 'been', 'not', 'they', 'it', 'have', 'will', 'has', 'by', 'for', 'start', 'one', 'thing', 'learned', 'im', 'youve', 'theyre', 'their', 'things', 'things', \"can't\", 'can', 'cant', 'will', 'go', 'going', 'let', 'lets', \"let's\", 'say', 'says', 'know', 'talk', 'talked', 'talks', 'dont', 'think', 'said', 'something', 'something', 'wont', 'people', 'make', 'want', 'went', 'goes', 'people', 'person', 'like', 'come', 'from', 'need', 'us', 'didnt', 'woudldnt', 'today', 'discuss', 'new', 'great', 'support', 'see', 'did', 'must', 'ha', 'wa', 'im', 'u', 'new', 'continue', 'tomorrow', 'join', 'everyone', 'time', 'great', 'may', 'see', 'area', 'good', 'across', 'take', 'tell', 'amazing', 'below', 'behind', 'next', 'hour', 'thank', 'thanks', 'come']\n"
     ]
    }
   ],
   "source": [
    "## get all stop words\n",
    "\n",
    "en_stop =[]\n",
    "en_stop1 = get_stop_words(\"english\")\n",
    "en_stop2 = stopwords.words('english')\n",
    "en_stop = [en_stop1,en_stop2,[\"youd\",\"doesnt\",\"youll\",\"will\",\"wont\",\"aint\",\"ith\",\"their\",\"they're\",\"is\",\n",
    "                              \"was\",\"been\",\"not\",\"they\",\"it\",\"have\",\"will\",\"has\",\"by\",\"for\",\"start\",\"one\",\n",
    "                              \"thing\",\"learned\",\"im\",\"youve\",\"theyre\",\"their\",\"things\" ,\"things\",\"can't\",\n",
    "                              \"can\",\"cant\",\"will\",\"go\",\"going\",\"let\",\"lets\",\"let's\",\"say\",\"says\",\"know\",\n",
    "                              \"talk\",\"talked\",\"talks\",\"dont\",\"think\",\"said\",\"something\",\"something\",\"wont\",\n",
    "                              \"people\",\"make\",\"want\",\"went\",\"goes\",\"people\",\"person\",\"like\",\"come\",\"from\",\n",
    "                              \"need\",\"us\",\"didnt\",\"woudldnt\",\"today\",\"discuss\",\"new\",\"great\",\"support\",\"see\",\n",
    "                              \"did\",\"must\",\"ha\",\"wa\",\"im\",\"u\",\"new\",\"continue\",\"tomorrow\",\"join\",\"everyone\",\n",
    "                              \"time\",\"great\",\"may\",\"see\",\"area\",\"good\",\"across\",\"take\",\"tell\",\"amazing\",\n",
    "                              \"below\",\"behind\",\"next\",\"hour\",\"thank\",\"thanks\",\"come\"]]\n",
    "en_stop = [item for sublist in en_stop for item in sublist]\n",
    "\n",
    "print(en_stop)\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_time(single_tweet):\n",
    "    month_dict ={'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n",
    "    \n",
    "    time_string = single_tweet['created_at']\n",
    "    all_time_items = time_string.split()\n",
    "    return month_dict[all_time_items[1]], all_time_items[2], all_time_items[-1]\n",
    "\n",
    "def preprocess(text):\n",
    "    clean_text = \"\"\n",
    "    for eachone in text:\n",
    "        try:\n",
    "            eachone.encode('ascii')\n",
    "            clean_text += eachone\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    new_text = []\n",
    "    for each_word in clean_text.split():\n",
    "        if each_word.isalpha():\n",
    "            lower_word = wordnet_lemmatizer.lemmatize(each_word.lower())\n",
    "            if lower_word not in en_stop:\n",
    "                new_text.append(lower_word)\n",
    "    return ' '.join(new_text)\n",
    "\n",
    "\n",
    "# disregarding retweets\n",
    "clean_tweet_dict   = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# load the dictionaries of tweets and combine them\n",
    "\n",
    "total_tweet_dict = pickle.load(open('layer_tweet_dict', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_raw_tweet_dict = dict()\n",
    "\n",
    "## Create the empty lists that willl hold our variables that we will bind into a dataframe\n",
    "region = []\n",
    "ceo = []\n",
    "name = []\n",
    "title = []\n",
    "office = []\n",
    "got = []\n",
    "party = []\n",
    "twitter_handle = []\n",
    "personal_handle = []\n",
    "IDlist = []\n",
    "author_id = []\n",
    "leo = []\n",
    "monthlist = []\n",
    "yearlist = []\n",
    "daylist = []\n",
    "totalArticle = []\n",
    "\n",
    "count = 0\n",
    "for key, layer_member in total_tweet_dict.items():\n",
    "    tweet_title = key\n",
    "    raw_tweet_list = layer_member.raw_twitter_record\n",
    "    \n",
    "    # call each tweet for each layer member\n",
    "    for each_tweet in raw_tweet_list:\n",
    "        # print(each_tweet)\n",
    "        \n",
    "        # keep only NON-retweets and NON-quote tweets\n",
    "        if not ('retweeted_status' in each_tweet or 'quoted_status' in each_tweet):\n",
    "            each_raw_tweet = each_tweet['full_text']\n",
    "            month, day, year = get_time(each_tweet)\n",
    "            \n",
    "            #clean each tweet using preprocess function from above\n",
    "            tmp_post_clean_tweet = preprocess(each_raw_tweet)\n",
    "            \n",
    "            if len(tmp_post_clean_tweet) > 0:\n",
    "                clean_raw_tweet_dict[tmp_post_clean_tweet] = each_raw_tweet\n",
    "                \n",
    "                region.append(layer_member.region)\n",
    "                ceo.append(layer_member.ceo)\n",
    "                name.append(layer_member.name)\n",
    "                title.append(layer_member.title)\n",
    "                office.append(layer_member.office)\n",
    "                got.append(layer_member.got)\n",
    "                party.append(layer_member.party)\n",
    "                twitter_handle.append(layer_member.twitter_handle)\n",
    "                personal_handle.append(layer_member.personalhandle)\n",
    "                IDlist.append(each_tweet['id'])\n",
    "                leo.append(layer_member.leo)\n",
    "                monthlist.append(month)\n",
    "                yearlist.append(year)\n",
    "                daylist.append(day)\n",
    "                totalArticle.append(tmp_post_clean_tweet)\n",
    "\n",
    "                try:\n",
    "                    author_id.append(each_tweet['author_id'])\n",
    "                except:\n",
    "                    author_id.append(each_tweet['user']['id_str'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116158\n"
     ]
    }
   ],
   "source": [
    "print(len(totalArticle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet               doc_id  \\\n",
      "0  secretary state office provide unofficial elec...  1539249005917614083   \n",
      "1  primary runoff election important voter voted ...  1537812869395406849   \n",
      "2  ahead june primary runoff secretary merrill re...  1537465239813468160   \n",
      "3  ahead june primary runoff secretary merrill re...  1537129963584901127   \n",
      "4  secretary merrill remind alabama voter last da...  1536364016464961536   \n",
      "\n",
      "         handle author_id region ceo         name               title  \\\n",
      "0  johnhmerrill  40788002  south      john merill  secretary of state   \n",
      "1  johnhmerrill  40788002  south      john merill  secretary of state   \n",
      "2  johnhmerrill  40788002  south      john merill  secretary of state   \n",
      "3  johnhmerrill  40788002  south      john merill  secretary of state   \n",
      "4  johnhmerrill  40788002  south      john merill  secretary of state   \n",
      "\n",
      "                                     office            got party  \\\n",
      "0  office of sectary of state john h merill  alasecofstate     r   \n",
      "1  office of sectary of state john h merill  alasecofstate     r   \n",
      "2  office of sectary of state john h merill  alasecofstate     r   \n",
      "3  office of sectary of state john h merill  alasecofstate     r   \n",
      "4  office of sectary of state john h merill  alasecofstate     r   \n",
      "\n",
      "  personal_handle  leo  month  year day  \n",
      "0                    1      6  2022  21  \n",
      "1                    1      6  2022  17  \n",
      "2                    1      6  2022  16  \n",
      "3                    1      6  2022  15  \n",
      "4                    1      6  2022  13  \n"
     ]
    }
   ],
   "source": [
    "# create dataset \n",
    "\n",
    "# df = pd.DataFrame(list(zip(totalArticle,IDlist,handle ,name,fname,lname,party,state,house,daylist,monthlist,yearlist)), \n",
    "#                columns =['tweet',\"doc_id\", \"handle\",'name',\"firstname\",\"lastname\",\"party\",\"state\",\"chamber\",\"day\",\"month\",\"year\"])\n",
    "# print(df.head())\n",
    "\n",
    "df = pd.DataFrame(list(zip(totalArticle,IDlist,twitter_handle,author_id,region,ceo,name,title,office,got,party,personal_handle,leo,monthlist,yearlist,daylist)), \n",
    "               columns=['tweet','doc_id','handle','author_id','region','ceo','name','title','office','got','party','personal_handle','leo','month','year','day'])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
